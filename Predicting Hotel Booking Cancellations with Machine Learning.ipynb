{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "701a401e-46b5-40f7-b7e5-b3b90f9a3ffd",
   "metadata": {},
   "source": [
    "# **Predictive Analysis of Hotel Booking Cancellations**\n",
    "\n",
    "### **Introduction**\n",
    "This study investigates hotel booking cancellations using machine learning techniques to identify the most effective predictive methods. The aim is to develop a model that accurately forecasts whether a booking will be cancelled based on crucial factors such as booking information, customer behaviour, and pricing details.\n",
    "\n",
    "### **Key Objectives**\n",
    "- To compare various predictive models, including **Logistic Regression, LDA, QDA, Ridge, and Lasso Regression**.\n",
    "- To analyse how **feature selection and regularisation** influence the performance of these models.\n",
    "- To explore **threshold tuning** to reduce misclassification risks and effectively manage hotel overbooking.\n",
    "- To evaluate whether reducing the number of predictors enhances or diminishes model accuracy.\n",
    "\n",
    "### **Methods**\n",
    "**Classification and regression methods** are employed, **cross-validation for model selection** is implemented, and **test errors and feature significance** are examined to inform our decisions.\n",
    "\n",
    "In this analysis, the objective is to identify the most efficient strategy for predicting hotel cancellations. This will assist hotels in streamlining operations and minimising revenue losses from no-shows**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61cb0f5c-7f37-4d67-aa09-947d06a2cab0",
   "metadata": {},
   "source": [
    "Preamble and Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13cd112e-c7ab-46db-b680-814a2b9ad44d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 119390 entries, 0 to 119389\n",
      "Data columns (total 20 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   hotel                           119390 non-null  object \n",
      " 1   is_canceled                     119390 non-null  int64  \n",
      " 2   lead_time                       119390 non-null  int64  \n",
      " 3   arrival_date_year               119390 non-null  int64  \n",
      " 4   arrival_date_month              119390 non-null  object \n",
      " 5   stays_in_weekend_nights         119390 non-null  int64  \n",
      " 6   stays_in_week_nights            119390 non-null  int64  \n",
      " 7   adults                          119390 non-null  int64  \n",
      " 8   children                        119386 non-null  float64\n",
      " 9   babies                          119390 non-null  int64  \n",
      " 10  country                         118902 non-null  object \n",
      " 11  distribution_channel            119390 non-null  object \n",
      " 12  is_repeated_guest               119390 non-null  int64  \n",
      " 13  previous_cancellations          119390 non-null  int64  \n",
      " 14  previous_bookings_not_canceled  119390 non-null  int64  \n",
      " 15  booking_changes                 119390 non-null  int64  \n",
      " 16  deposit_type                    119390 non-null  object \n",
      " 17  adr                             119390 non-null  float64\n",
      " 18  required_car_parking_spaces     119390 non-null  int64  \n",
      " 19  total_of_special_requests       119390 non-null  int64  \n",
      "dtypes: float64(2), int64(13), object(5)\n",
      "memory usage: 18.2+ MB\n",
      "None\n",
      "          hotel  is_canceled  lead_time  arrival_date_year arrival_date_month  \\\n",
      "0  Resort Hotel            0        342               2015               July   \n",
      "1  Resort Hotel            0        737               2015               July   \n",
      "2  Resort Hotel            0          7               2015               July   \n",
      "3  Resort Hotel            0         13               2015               July   \n",
      "4  Resort Hotel            0         14               2015               July   \n",
      "\n",
      "   stays_in_weekend_nights  stays_in_week_nights  adults  children  babies  \\\n",
      "0                        0                     0       2       0.0       0   \n",
      "1                        0                     0       2       0.0       0   \n",
      "2                        0                     1       1       0.0       0   \n",
      "3                        0                     1       1       0.0       0   \n",
      "4                        0                     2       2       0.0       0   \n",
      "\n",
      "  country distribution_channel  is_repeated_guest  previous_cancellations  \\\n",
      "0     PRT               Direct                  0                       0   \n",
      "1     PRT               Direct                  0                       0   \n",
      "2     GBR               Direct                  0                       0   \n",
      "3     GBR            Corporate                  0                       0   \n",
      "4     GBR                TA/TO                  0                       0   \n",
      "\n",
      "   previous_bookings_not_canceled  booking_changes deposit_type   adr  \\\n",
      "0                               0                3   No Deposit   0.0   \n",
      "1                               0                4   No Deposit   0.0   \n",
      "2                               0                0   No Deposit  75.0   \n",
      "3                               0                0   No Deposit  75.0   \n",
      "4                               0                0   No Deposit  98.0   \n",
      "\n",
      "   required_car_parking_spaces  total_of_special_requests  \n",
      "0                            0                          0  \n",
      "1                            0                          0  \n",
      "2                            0                          0  \n",
      "3                            0                          0  \n",
      "4                            0                          1  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Dataset is in same folder as notebook\n",
    "hotel_bookings = pd.read_csv(\"hotel_booking.csv\")\n",
    "\n",
    "# Displays dataset information\n",
    "print(hotel_bookings.info())\n",
    "print(hotel_bookings.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b48c4d-2111-4c4e-9edc-68883dae1d66",
   "metadata": {},
   "source": [
    "#  Step 1: Data Cleaning - Addressing Missing Values\n",
    "\n",
    "To create a clean dataset for analysis, I will **detect and eliminate any observations with missing data**.  \n",
    "Missing information can cause bias and negatively impact model performance; therefore, removing these entries guarantees **more dependable outcomes**.  \n",
    "\n",
    "This phase is essential for the subsequent modelling tasks (b) to (f), as incomplete data might cause errors in regression and classification models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0191e1eb-3e50-4645-879c-702890f2a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing values after removal:\n",
      "hotel                             0\n",
      "is_canceled                       0\n",
      "lead_time                         0\n",
      "arrival_date_year                 0\n",
      "arrival_date_month                0\n",
      "stays_in_weekend_nights           0\n",
      "stays_in_week_nights              0\n",
      "adults                            0\n",
      "children                          0\n",
      "babies                            0\n",
      "country                           0\n",
      "distribution_channel              0\n",
      "is_repeated_guest                 0\n",
      "previous_cancellations            0\n",
      "previous_bookings_not_canceled    0\n",
      "booking_changes                   0\n",
      "deposit_type                      0\n",
      "adr                               0\n",
      "required_car_parking_spaces       0\n",
      "total_of_special_requests         0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Removing rows with missing values\n",
    "hotel_bookings.dropna(inplace=True)\n",
    "\n",
    "# Making sure the missing values have been removed\n",
    "print(\"\\nMissing values after removal:\")\n",
    "print(hotel_bookings.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c33a65d-9c9f-408a-93d8-19f17c1fd4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting binary variables\n",
    "hotel_bookings['is_canceled'] = hotel_bookings['is_canceled'].astype(int)\n",
    "hotel_bookings['is_repeated_guest'] = hotel_bookings['is_repeated_guest'].astype(int)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "94db2924-35ba-451d-812f-1417b928fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the Categorical variables have been converted successfully !!\n"
     ]
    }
   ],
   "source": [
    "# Converting  categorical variable (arrival_date_month) to numeric\n",
    "month_mapping = {'January': 1, 'February': 2, 'March': 3, 'April': 4, 'May': 5, 'June': 6,\n",
    "                 'July': 7, 'August': 8, 'September': 9, 'October': 10, 'November': 11, 'December': 12}\n",
    "hotel_bookings['arrival_date_month'] = hotel_bookings['arrival_date_month'].map(month_mapping)\n",
    "\n",
    "# encoding categorical variables\n",
    "hotel_bookings = pd.get_dummies(hotel_bookings, columns=['hotel', 'deposit_type', 'distribution_channel', 'country'], drop_first=True)\n",
    "\n",
    "#clarification \n",
    "print(\"the Categorical variables have been converted successfully !!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0aa89d-484a-4edc-ac3e-e1511520d997",
   "metadata": {},
   "source": [
    "# Step 2: Splitting Data into Training and Test Sets\n",
    "\n",
    "To evaluate model performance, the dataset will be **randomly split into two equal parts**:\n",
    "- **50% Training Set** â†’ Used for model training.\n",
    "- **50% Test Set** â†’ Used to assess model performance on unseen data.\n",
    "\n",
    "## ðŸ”¹ Ensuring Reproducibility:\n",
    "To maintain **consistent results**, a **random seed** will be set using the student number.  \n",
    "This ensures that the data split remains the same every time the code is executed, allowing for reproducible and fair comparisons between models.\n",
    "\n",
    "This step prevents **data leakage** and ensures that models generalize well to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "285988e5-15c5-4578-bfdc-162419e85d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Setting the random seed\n",
    "np.random.seed(38474328)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "284bae86-f43a-4087-8093-2c1c698399b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set size: 59449 samples\n",
      "Test set size: 59449 samples\n"
     ]
    }
   ],
   "source": [
    "# Defining features (X) and target variable (y)\n",
    "X = hotel_bookings.drop(columns=['is_canceled'])  \n",
    "y = hotel_bookings['is_canceled'] \n",
    "\n",
    "# Dividing the dataset into 50% for training and 50% for testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=38242328)\n",
    "\n",
    "#clarification\n",
    "print(f\"Training set size: {X_train.shape[0]} samples\")\n",
    "print(f\"Test set size: {X_test.shape[0]} samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b70d0a-4fad-4c1b-8995-ddb796e6c741",
   "metadata": {},
   "source": [
    "#  Step 3: Linear vs. Logistic Regression for Predicting Cancellations\n",
    "\n",
    "To predict whether a **hotel booking will be cancelled**, two regression models will be fitted:\n",
    "\n",
    "- **Linear Regression (Ordinary Least Squares, OLS)** is typically used for continuous variables. It will be applied to evaluate its effectiveness in a binary classification problem.\n",
    "- **Logistic Regression**: Designed for binary classification, this model is expected to better predict cancellations better.\n",
    "  \n",
    "## Model Evaluation:\n",
    "- **Mean Squared Error (MSE) for Linear Regression** â†’ Measures prediction errors.\n",
    "- **Accuracy Score for Logistic Regression** â†’ Assesses classification performance.\n",
    "\n",
    "##  Objective:\n",
    "- Compare the **test errors** of both models.\n",
    "- Determine which model is **better suited** for this task.\n",
    "- Highlight key insights on **model performance and suitability for classification problems**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2f5ae982-1a8a-4787-8de5-ee63cd42ae0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e8a2bbbd-213a-4dd2-b900-b9297657006f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (Linear Regression): 0.1476\n"
     ]
    }
   ],
   "source": [
    "# OLS model\n",
    "linear_model = LinearRegression()\n",
    "linear_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "y_pred_linear = linear_model.predict(X_test)\n",
    "\n",
    "# Mean Squared Error (MSE)\n",
    "mse_linear = mean_squared_error(y_test, y_pred_linear)\n",
    "print(f\"Mean Squared Error (Linear Regression): {mse_linear:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dee5693d-2635-4170-ae59-e7196587bda6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (Logistic Regression - Scaled): 0.7968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data\n",
    "X_test_scaled = scaler.transform(X_test)  # Transform test data (use same scaler)\n",
    "\n",
    "# Fit Logistic Regression again\n",
    "logistic_model = LogisticRegression(max_iter=2000)\n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# Predict & check accuracy\n",
    "y_pred_logistic = logistic_model.predict(X_test_scaled)\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "print(f\"Accuracy (Logistic Regression - Scaled): {accuracy_logistic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce88055a-61f9-4913-973c-c13bf634948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¹ Linear Regression - MSE: 0.1476\n",
      "ðŸ”¹ Logistic Regression - Accuracy: 0.7968\n"
     ]
    }
   ],
   "source": [
    "print(f\"ðŸ”¹ Linear Regression - MSE: {mse_linear:.4f}\")\n",
    "print(f\"ðŸ”¹ Logistic Regression - Accuracy: {accuracy_logistic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c026634-246a-4251-a1b0-36d2a83cd7e9",
   "metadata": {},
   "source": [
    "## **Comparison of Test Errors: Linear vs. Logistic Regression**\n",
    "\n",
    "### **Key Findings**\n",
    "| Model                  | Test Metric |\n",
    "|------------------------|------------|\n",
    "| **Linear Regression**  | MSE = **0.1476** |\n",
    "| **Logistic Regression** | Accuracy = **79.68%** |\n",
    "\n",
    "### **Why Logistic Regression Performs Better**\n",
    "- **Linear Regression (OLS)** operates under the assumption that the target variable is continuous, which makes it **inappropriate for classification problems** where the outcome is binary (0 or 1).  \n",
    "- The **low MSE (0.1476) for Linear Regression does not equate to effective classification performance**, as MSE is not an optimal metric for categorical predictions.\n",
    "- **Logistic Regression (Accuracy: 79.68%) is notably more efficient** in predicting cancellations because:\n",
    "  - It **estimates probabilities** instead of continuous outcomes.\n",
    "  - It **allocates probabilities to each class**, which enables threshold adjustments to reduce classification errors.\n",
    "  - The model **successfully differentiates between â€œcancelledâ€ and â€œnot cancelledâ€ bookings**.\n",
    "\n",
    "### **Key Takeaways**\n",
    "- **Logistic Regression is the favoured model for this task**, as it appropriately addresses binary classification.\n",
    "- **Linear Regression is not intended for categorical results**, and even though MSE may appear low, it does not accurately represent actual classification performance.\n",
    "- **Feature scaling considerably enhanced Logistic Regressionâ€™s accuracy**, ensuresimproved predictions.\n",
    "\n",
    "\n",
    "\n",
    "To predict cancellations, **Logistic Regression emerges as the superior model**, as it efficiently addresses binary classification and achieves a commendable accuracy of **79.68%**. Scaling the features before training further boosted model performance, establishing it as a critical preprocessing step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c0a70c-6620-4370-ac68-abc1b94c0d96",
   "metadata": {},
   "source": [
    "#  Step 4: Adjusting Classification Threshold to Minimize Overbooking Risk\n",
    "\n",
    "The standard **Bayes Classifier** assigns observations to the most likely class using a **50% probability threshold**. However, this approach may not be optimal for the hotel company, which faces a **specific misclassification concern**:  \n",
    "\n",
    "- **Overbooking Risk**: Some customers predicted to cancel may show up, leading to more bookings than available rooms.\n",
    "\n",
    "## Adjusting the Threshold:\n",
    "To address this issue, the classification threshold will be modified:\n",
    "- **0.5 (Default)**: Standard probability threshold.\n",
    "- **0.6 (More Conservative)**: Only high-certainty cancellations are counted.\n",
    "- **0.7 (Stricter)**: Reduces false cancellations but increases the risk of missing actual cancellations.\n",
    "\n",
    "## Objective:\n",
    "- Evaluate how the **adjusted threshold** affects classification outcomes.\n",
    "- Compare accuracy, false positives, and false negatives across different thresholds.\n",
    "- Assess how these changes align with the **hotel companyâ€™s goal** of minimizing overbooking risks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "721ccdca-9dd9-4f75-9baf-58bd64f81a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Converged! Accuracy: 0.7968\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train) \n",
    "X_test_scaled = scaler.transform(X_test)  \n",
    "\n",
    "\n",
    "logistic_model = LogisticRegression(solver='saga', max_iter=5000) \n",
    "logistic_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "\n",
    "y_prob_logistic = logistic_model.predict_proba(X_test_scaled)[:, 1]  \n",
    "\n",
    "# Evaluate performance\n",
    "y_pred_logistic = (y_prob_logistic > 0.5).astype(int)  # Default threshold\n",
    "accuracy_logistic = accuracy_score(y_test, y_pred_logistic)\n",
    "\n",
    "print(f\"Logistic Regression Converged! Accuracy: {accuracy_logistic:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3427d9ce-90fa-4c3e-b32f-a465aa5017b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, different the three thresholds\n",
    "thresholds = [0.5, 0.6, 0.7]\n",
    "\n",
    "# Store results\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    # Converting the  probabilities to class labels based on the threshold\n",
    "    y_pred_adjusted = np.where(y_prob_logistic > threshold, 1, 0)\n",
    "    \n",
    "    # accuracy\n",
    "    accuracy = accuracy_score(y_test, y_pred_adjusted)\n",
    "    \n",
    "    #  confusion matrix\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred_adjusted).ravel()\n",
    "    \n",
    "    # Store results\n",
    "    results.append((threshold, accuracy, tn, fp, fn, tp))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "aeab5b69-5c4a-4858-91b0-3771763ef222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Threshold</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>True Negative</th>\n",
       "      <th>False Positive</th>\n",
       "      <th>False Negative</th>\n",
       "      <th>True Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.791889</td>\n",
       "      <td>34382</td>\n",
       "      <td>2807</td>\n",
       "      <td>9565</td>\n",
       "      <td>12695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.783832</td>\n",
       "      <td>36057</td>\n",
       "      <td>1132</td>\n",
       "      <td>11719</td>\n",
       "      <td>10541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.775623</td>\n",
       "      <td>36796</td>\n",
       "      <td>393</td>\n",
       "      <td>12946</td>\n",
       "      <td>9314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Threshold  Accuracy  True Negative  False Positive  False Negative  \\\n",
       "0        0.5  0.791889          34382            2807            9565   \n",
       "1        0.6  0.783832          36057            1132           11719   \n",
       "2        0.7  0.775623          36796             393           12946   \n",
       "\n",
       "   True Positive  \n",
       "0          12695  \n",
       "1          10541  \n",
       "2           9314  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Threshold  Accuracy  True Negative  False Positive  False Negative  \\\n",
      "0        0.5  0.791889          34382            2807            9565   \n",
      "1        0.6  0.783832          36057            1132           11719   \n",
      "2        0.7  0.775623          36796             393           12946   \n",
      "\n",
      "   True Positive  \n",
      "0          12695  \n",
      "1          10541  \n",
      "2           9314  \n"
     ]
    }
   ],
   "source": [
    "df_results = pd.DataFrame(results, columns=['Threshold', 'Accuracy', 'True Negative', 'False Positive', 'False Negative', 'True Positive'])\n",
    "\n",
    "\n",
    "display(df_results)  \n",
    "print(df_results)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747b2f1-bee7-4042-9dfa-b012ecad5e19",
   "metadata": {},
   "source": [
    "\n",
    "###  Main Findings\n",
    "| Threshold | Accuracy | True Negative | False Positive | False Negative | True Positive |\n",
    "|-----------|-----------|---------------|---------------|---------------|--------------|\n",
    "| **0.5 (Standard)** | **79.19%** | 34,382 | 2,807 | 9,565 | 12,695 |\n",
    "| **0.6** | **78.38%** | 36,057 | 1,132 | 11,719 | 10,541 |\n",
    "| **0.7** | **77.56%** | 36,796 | **393** | **12,946** | 9,314 |\n",
    "\n",
    "### How Does Changing the Threshold Affect Predictions?\n",
    "1. **Threshold = 0.5 (Standard)**  \n",
    "   - **Balances** false positives and false negatives.\n",
    "   - 2,807 **False Positives** (cancellations predicted incorrectly).\n",
    "   - 9,565 **False Negatives** (actual cancellations not identified).\n",
    "   - **Optimal for general prediction accuracy** but may still lead to overbooking issues.\n",
    "\n",
    "2. **Threshold = 0.6 (Cautious Approach)**  \n",
    "   - **Lowers False Positives (1,132), decreasing erroneous cancellations.**\n",
    "   - **Raises False Negatives (11,719), meaning more genuine cancellations go unnoticed.**\n",
    "   - **Reduced risk of overbooking** but might result in vacant hotel rooms.\n",
    "\n",
    "3. **Threshold = 0.7 (Most Cautious, Stringent Policy)**  \n",
    "   - **Significantly decreases False Positives (393), reducing incorrect cancellations.**\n",
    "   - **Highest False Negatives (12,946), indicating the hotel may misjudge demand.**\n",
    "   - **Enhances reliability in cancellation predictions but could lead to unoccupied rooms due to underestimated demand.**\n",
    "\n",
    "### Implications & Alignment with Hotel Objectives\n",
    "If overall accuracy is the aim, a lower threshold (0.5) is preferable, because it captures a more significant number of actual cancellations.  \n",
    "- **A higher threshold (0.7) is advantageous for reducing false positives and minimizing  cancellations and overbooking risks.  \n",
    "- **Threshold = 0.6 provides a compromise**, balancing accuracy while at the same time mitigating overbooking risk.\n",
    "\n",
    "### Recommendation\n",
    "The **most suitable threshold is determined by the hotelâ€™s priority**:\n",
    "- If the primary goal is to prevent **overbooking, a threshold of 0.6 or 0.7** would be more suitable.\n",
    "- If maximizing room occupancy while still achieving good accuracy is preferred, **maintaining a threshold of 0.5** would be ideal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234a34ed-e049-4067-a2d3-4135adc315ec",
   "metadata": {},
   "source": [
    "# Step 5: Linear Discriminant Analysis (LDA) & Quadratic Discriminant Analysis (QDA)\n",
    "\n",
    "To predict whether a **hotel booking will be canceled**, two classification models will be applied:\n",
    "\n",
    "**Linear Discriminant Analysis (LDA)**: Assumes that the predictor variables follow a normal distribution with the same covariance structure across classes.\n",
    "- **Quadratic Discriminant Analysis (QDA)**: Similar to LDA but allows each class to have a different covariance structure, making it more flexible for capturing non-linear relationships.\n",
    "\n",
    "## Model Evaluation:\n",
    "- The **test error** for both LDA and QDA will be computed.\n",
    "- Performance will be compared against **logistic regression and threshold-adjusted classification models** from Step 5.\n",
    "\n",
    "## Objective:\n",
    "- Assess how well LDA and QDA perform in predicting cancellations.\n",
    "- Determine whether **linear or quadratic classification is more effective**.\n",
    "- Compare findings with previous classification models to understand which approach provides the **best predictive accuracy**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34f801c-d943-490f-8fcd-bbcd58b193c1",
   "metadata": {},
   "source": [
    "In the dataset, some predictor variables are **strongly correlated (collinear)**. This creates challenges for models such as **LDA (Linear Discriminant Analysis) and QDA (Quadratic Discriminant Analysis)** due to the following reasons:\n",
    "- **Collinear variables provide redundant information**, which can destabilize the model.\n",
    "LDA and QDA presume that the predictor variables are not considerably correlated, so multicollinearity can hinder their effectiveness.\n",
    "- **The covariance matrix may become singular**, meaning the model struggles to establish decision boundaries accurately.\n",
    "\n",
    "### **Proposed Solution: Principal Component Analysis (PCA)**\n",
    "Rather than manually eliminating features, we utilize **Principal Component Analysis (PCA)**, which:\n",
    "1. **Lowers dimensionality** by converting numerous correlated features into a fewer number of uncorrelated \"principal components\".\n",
    "2. **Preserves as much information as possible** while eliminating redundancy.\n",
    "3. **Enhances stability** for LDA & QDA by ensuring that the covariance matrix is well-defined.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c713eaec-f5c5-469f-adda-7880732ee57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "pca = PCA(n_components=10)  \n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db70884-b409-43d5-90eb-5d62f790a132",
   "metadata": {},
   "source": [
    "LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7a27c2e8-167c-44b3-84d4-2aca9d1559f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA Accuracy: 0.7310\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "lda_model_pca = LinearDiscriminantAnalysis()\n",
    "lda_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "y_pred_lda_pca = lda_model_pca.predict(X_test_pca)\n",
    "\n",
    "\n",
    "accuracy_lda_pca = accuracy_score(y_test, y_pred_lda_pca)\n",
    "print(f\"LDA Accuracy: {accuracy_lda_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0583ef60-2264-4f29-bc9b-9cbab7aaa50c",
   "metadata": {},
   "source": [
    "QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8a5f0872-59bf-470b-8d80-0d237ea92d73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QDA Accuracy: 0.7259\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "\n",
    "\n",
    "qda_model_pca = QuadraticDiscriminantAnalysis()\n",
    "qda_model_pca.fit(X_train_pca, y_train)\n",
    "\n",
    "\n",
    "y_pred_qda_pca = qda_model_pca.predict(X_test_pca)\n",
    "\n",
    "\n",
    "accuracy_qda_pca = accuracy_score(y_test, y_pred_qda_pca)\n",
    "print(f\"QDA Accuracy: {accuracy_qda_pca:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcfe7dd9-cdb6-4676-979f-0d1464a4c667",
   "metadata": {},
   "source": [
    "## **Assessment of LDA and QDA Models**\n",
    "| Model | Accuracy |\n",
    "|--------|----------|\n",
    "| **LDA** | **73.10%** |\n",
    "| **QDA** | **72.59%** |\n",
    "| **Logistic Regression (Default 50% Threshold)** | **79.19%** |\n",
    "| **Logistic Regression (60% Threshold)** | **78.38%** |\n",
    "| **Logistic Regression (70% Threshold)** | **77.56%** |\n",
    "\n",
    "### **Analyzing LDA vs. QDA Results**\n",
    "1. **LDA (Accuracy: 73.10%)**\n",
    "   - LDA operates under the assumption that **the covariance matrix is consistent across both classes**, making it more effective when the data is **linearly separable**.\n",
    "   - Despite LDA's solid performance, it is marginally **less efficient than Logistic Regression (79.19%)**.\n",
    "\n",
    "2. **QDA (Accuracy: 72.59%)**\n",
    "   - QDA accommodates **distinct covariance matrices** for each class, resulting in greater flexibility than LDA.\n",
    "   - Nevertheless, this added flexibility can also lead to **overfitting**, particularly with smaller datasets.\n",
    "   - QDA's performance, being slightly **inferior to LDA**, suggests that a **linear decision boundary** is more appropriate for this dataset.\n",
    "\n",
    "### **Logistic Regression Comparison**\n",
    "- **Logistic Regression (79.19%) surpasses both LDA (73.10%) and QDA (72.59%)**.\n",
    "- The Logistic Regression increases when **feature scaling is applied effectively** (as performed during preprocessing).\n",
    "- While LDA and QDA offer **probabilistic classification**, Logistic Regression's optimization of the decision boundary via **maximum likelihood estimation** makes it **more suitable for binary classification tasks**, such as forecasting cancellations.\n",
    "\n",
    "### **Effects of Threshold Adjustment in Logistic Regression**\n",
    "- Modifying the threshold in Logistic Regression created an opportunity to **manage false positives and negatives**, which is vital for reducing **overbooking risks**.\n",
    "Unlike Logistic Regression, LDA and QDA do not permit threshold modifications similarly, making them less adaptable to business requirements.\n",
    "\n",
    "### Key Takeaways\n",
    "- **Logistic Regression is the top-performing model overall (79.19%)**.\n",
    "- **LDA is a viable alternative (73.10%)** but lacks threshold tuning capabilities.\n",
    "- **QDA (72.59%) shows a higher tendency for overfitting**, indicating that assuming **a unified covariance structure (LDA) is preferable** for this dataset.\n",
    "- **Logistic Regression with an adjusted threshold (e.g., 60%)** represents the optimal choice for the hotel seeking a balance between accuracy and clarity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a9bd04-8dd2-4c54-b5b8-5c0f9221ddac",
   "metadata": {},
   "source": [
    "# Step 6: Ridge & Lasso Regression for Predicting Cancellations\n",
    "\n",
    "To improve prediction accuracy and prevent overfitting, **Ridge** and **Lasso Regression** will be applied:\n",
    "\n",
    "- **Ridge Regression (L2 Regularization)**: Shrinks coefficients but **retains all predictors**, making it useful when predictors are highly correlated.\n",
    "- **Lasso Regression (L1 Regularization)**: Shrinks coefficients and **eliminates less important features**, performing automatic feature selection.\n",
    "\n",
    "## Cross-Validation for Î» Selection:\n",
    "- The regularization strength (**Î»**) will be selected using **5-fold and 10-fold cross-validation**.\n",
    "- The optimal **Î»** will be chosen based on test error minimization.\n",
    "\n",
    "## Model Evaluation:\n",
    "- **Mean Squared Error (MSE)** for Ridge and Lasso models.\n",
    "- **Number of non-zero coefficients in Lasso** as a function of Î».\n",
    "- Comparison of results with previous models.\n",
    "\n",
    "## Objective:\n",
    "- Assess whether **regularization improves predictive performance**.\n",
    "- Determine if **Lassoâ€™s feature selection** enhances model efficiency.\n",
    "- Compare results with previous classification models to identify the **best approach for cancellation prediction**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "208bd59e-b789-4623-815b-2680aaa04bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "2b72a3f4-e935-44c6-b8fd-209a883b4eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_grid = np.logspace(-3, 3, 10)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "b5d9ec9e-4854-4674-8efc-a9540cc49f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Î» (5-fold Ridge): 2.154434690031882\n",
      "Best Î» (10-fold Ridge): 2.154434690031882\n",
      "MSE (5-fold Ridge): 0.1475\n",
      "MSE (10-fold Ridge): 0.1475\n",
      "Best Î» (5-fold Lasso): 0.001\n",
      "Best Î» (10-fold Lasso): 0.001\n",
      "MSE (5-fold Lasso): 0.1496\n",
      "MSE (10-fold Lasso): 0.1496\n",
      "Nonzero Coefficients (5-fold Lasso): 27\n",
      "Nonzero Coefficients (10-fold Lasso): 27\n"
     ]
    }
   ],
   "source": [
    "ridge_5cv = RidgeCV(alphas=lambda_grid, store_cv_results=True)\n",
    "ridge_5cv.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "ridge_10cv = RidgeCV(alphas=lambda_grid, store_cv_results=True)\n",
    "ridge_10cv.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ridge_5cv = ridge_5cv.predict(X_test)\n",
    "y_pred_ridge_10cv = ridge_10cv.predict(X_test)\n",
    "mse_ridge_5cv = mean_squared_error(y_test, y_pred_ridge_5cv)\n",
    "mse_ridge_10cv = mean_squared_error(y_test, y_pred_ridge_10cv)\n",
    "lasso_5cv = LassoCV(alphas=lambda_grid, cv=5, max_iter=5000)\n",
    "lasso_5cv.fit(X_train, y_train)\n",
    "lasso_10cv = LassoCV(alphas=lambda_grid, cv=10, max_iter=5000)\n",
    "lasso_10cv.fit(X_train, y_train)\n",
    "y_pred_lasso_5cv = lasso_5cv.predict(X_test)\n",
    "y_pred_lasso_10cv = lasso_10cv.predict(X_test)\n",
    "\n",
    "\n",
    "mse_lasso_5cv = mean_squared_error(y_test, y_pred_lasso_5cv)\n",
    "mse_lasso_10cv = mean_squared_error(y_test, y_pred_lasso_10cv)\n",
    "\n",
    "\n",
    "num_nonzero_lasso_5cv = np.sum(lasso_5cv.coef_ != 0)\n",
    "num_nonzero_lasso_10cv = np.sum(lasso_10cv.coef_ != 0)\n",
    "\n",
    "#  results\n",
    "print(f\"Best Î» (5-fold Ridge): {ridge_5cv.alpha_}\")\n",
    "print(f\"Best Î» (10-fold Ridge): {ridge_10cv.alpha_}\")\n",
    "print(f\"MSE (5-fold Ridge): {mse_ridge_5cv:.4f}\")\n",
    "print(f\"MSE (10-fold Ridge): {mse_ridge_10cv:.4f}\")\n",
    "\n",
    "print(f\"Best Î» (5-fold Lasso): {lasso_5cv.alpha_}\")\n",
    "print(f\"Best Î» (10-fold Lasso): {lasso_10cv.alpha_}\")\n",
    "print(f\"MSE (5-fold Lasso): {mse_lasso_5cv:.4f}\")\n",
    "print(f\"MSE (10-fold Lasso): {mse_lasso_10cv:.4f}\")\n",
    "print(f\"Nonzero Coefficients (5-fold Lasso): {num_nonzero_lasso_5cv}\")\n",
    "print(f\"Nonzero Coefficients (10-fold Lasso): {num_nonzero_lasso_10cv}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9799f8bb-4c98-44b1-be5c-c831afe57046",
   "metadata": {},
   "source": [
    "Compare results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ec204bc3-5c34-48ac-b2c7-fb299e018ff0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Best Î»</th>\n",
       "      <th>MSE</th>\n",
       "      <th>Nonzero Coefficients</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ridge (5-fold)</td>\n",
       "      <td>2.154435</td>\n",
       "      <td>0.147542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ridge (10-fold)</td>\n",
       "      <td>2.154435</td>\n",
       "      <td>0.147542</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Lasso (5-fold)</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.149553</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lasso (10-fold)</td>\n",
       "      <td>0.001000</td>\n",
       "      <td>0.149553</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Model    Best Î»       MSE  Nonzero Coefficients\n",
       "0   Ridge (5-fold)  2.154435  0.147542                   NaN\n",
       "1  Ridge (10-fold)  2.154435  0.147542                   NaN\n",
       "2   Lasso (5-fold)  0.001000  0.149553                  27.0\n",
       "3  Lasso (10-fold)  0.001000  0.149553                  27.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Model    Best Î»       MSE  Nonzero Coefficients\n",
      "0   Ridge (5-fold)  2.154435  0.147542                   NaN\n",
      "1  Ridge (10-fold)  2.154435  0.147542                   NaN\n",
      "2   Lasso (5-fold)  0.001000  0.149553                  27.0\n",
      "3  Lasso (10-fold)  0.001000  0.149553                  27.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# Create a DataFrame to store results\n",
    "ridge_lasso_results = pd.DataFrame({\n",
    "    \"Model\": [\"Ridge (5-fold)\", \"Ridge (10-fold)\", \"Lasso (5-fold)\", \"Lasso (10-fold)\"],\n",
    "    \"Best Î»\": [ridge_5cv.alpha_, ridge_10cv.alpha_, lasso_5cv.alpha_, lasso_10cv.alpha_],\n",
    "    \"MSE\": [mse_ridge_5cv, mse_ridge_10cv, mse_lasso_5cv, mse_lasso_10cv],\n",
    "    \"Nonzero Coefficients\": [None, None, num_nonzero_lasso_5cv, num_nonzero_lasso_10cv]\n",
    "})\n",
    "\n",
    "# Display the results\n",
    "display(ridge_lasso_results)\n",
    "print(ridge_lasso_results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5806ee93-04a3-492e-85a4-08e0c1a3e4ac",
   "metadata": {},
   "source": [
    "## **Ridge and Lasso Regression for Predicting Cancellations**\n",
    "\n",
    "### **Results Summary**\n",
    "| Model | Best Î» | MSE | Nonzero Coefficients |\n",
    "|--------|--------|------|---------------------|\n",
    "| **Ridge (5-fold)** | 2.1544 | 0.1475 | N/A |\n",
    "| **Ridge (10-fold)** | 2.1544 | 0.1475 | N/A |\n",
    "| **Lasso (5-fold)** | 0.0010 | 0.1496 | 27 |\n",
    "| **Lasso (10-fold)** | 0.0010 | 0.1496 | 27 |\n",
    "\n",
    "---\n",
    "\n",
    "### **Ridge vs. Lasso: Key Findings**\n",
    "#### **Ridge Regression**\n",
    "- **Optimal Î» (Regularization Strength):** **2.1544** (identical for both 5-fold and 10-fold CV).  \n",
    "- **MSE:** **0.1475**reflects a slightly lower test error than Lasso.  \n",
    "- **Feature Retention:** **Maintains all predictors** (Ridge does not set coefficients to zero).  \n",
    "- **Conclusion:** Ridge is advantageous when we suspect **multicollinearity** among variables, as it stabilizes the Regression by shrinking coefficient values.\n",
    "\n",
    "#### **Lasso Regression**\n",
    "- **Optimal Î»:** **0.0010** (identical for both 5-fold and 10-fold CV).  \n",
    "- **MSE:** **0.1496**, marginally greater than Ridge, indicating some bias is introduced.  \n",
    "- **Feature Reduction:** **Lasso narrowed the model down to just 27 nonzero coefficients**, effectively discarding irrelevant predictors.  \n",
    "Conclusion: Lasso is valuable for automatic feature selection, as it simplifies the model by eliminating some coefficients.\n",
    "\n",
    "---\n",
    "\n",
    "### **Interpretation of Results**\n",
    "- Ridge Regression yielded a slightly lower **Mean Squared Error (MSE)** than Lasso, suggesting it **has a slight edge in predictive performance**.\n",
    "- Conversely, Lasso Regression **removed unnecessary variables**, making it a superior option for **model interpretability and mitigating overfitting**.\n",
    "- The **optimal selection hinges on the business context**:\n",
    "  - If a **more transparent model** is required, Lasso is preferable.\n",
    "  - Ridge is better if the concern is  **multicollinearity** and a **stable model that retains all features** is preferred.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e9549b-b148-40df-9d2f-ca93d9842425",
   "metadata": {},
   "source": [
    "#  Step 7: Model Re-Evaluation with Selected Predictors\n",
    "\n",
    "To assess the impact of **feature selection**, models will be retrained using a **subset of five predictors** chosen from the dataset.  \n",
    "\n",
    "## Steps:\n",
    "1. **Manually select five predictors** and reapply:\n",
    "   - **Linear Regression**\n",
    "   - **Logistic Regression**\n",
    "   - **LDA & QDA**\n",
    "   - **Ridge & Lasso Regression**\n",
    "2. **Compare classification errors (accuracy, MSE) with full-model results** to determine whether reducing predictors improves or worsens performance.\n",
    "\n",
    "## Feature Selection using Lasso:\n",
    "- Instead of manually selecting predictors, an additional experiment will be conducted using **the top features chosen by Lasso Regression (from Step 7)**.\n",
    "- The exact models will be applied to see if **Lasso-based feature selection improves performance**.\n",
    "\n",
    "## Objective:\n",
    "- Evaluate whether reducing the number of predictors improves **efficiency and interpretability**.\n",
    "- Compare model performance with **all predictors vs. selected predictors**.\n",
    "- Determine if **Lassoâ€™s feature selection provides a better-performing subset** for predictive modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38dbf2cf-f783-498d-9f58-448bbb59580a",
   "metadata": {},
   "source": [
    "Selecting 5 Predictors Manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "231627fa-d45f-4b13-84ba-1003273f3924",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_features = ['lead_time', 'adr', 'previous_cancellations', 'stays_in_week_nights', 'is_repeated_guest']\n",
    "X_train_reduced = X_train[selected_features]\n",
    "X_test_reduced = X_test[selected_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3a90b2-4e0b-4ff2-812e-fa7833afb11c",
   "metadata": {},
   "source": [
    "A. Linear & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "09e1245b-3844-43d3-aeca-6de0c175de83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE (Linear Regression - Reduced Features): 0.2105\n",
      "Accuracy (Logistic Regression - Reduced Features): 0.6845\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Linear Regression\n",
    "linear_model_reduced = LinearRegression()\n",
    "linear_model_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_linear_reduced = linear_model_reduced.predict(X_test_reduced)\n",
    "mse_linear_reduced = mean_squared_error(y_test, y_pred_linear_reduced)\n",
    "\n",
    "# Logistic Regression\n",
    "logistic_model_reduced = LogisticRegression(max_iter=1000)\n",
    "logistic_model_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_logistic_reduced = logistic_model_reduced.predict(X_test_reduced)\n",
    "accuracy_logistic_reduced = accuracy_score(y_test, y_pred_logistic_reduced)\n",
    "\n",
    "print(f\"MSE (Linear Regression - Reduced Features): {mse_linear_reduced:.4f}\")\n",
    "print(f\"Accuracy (Logistic Regression - Reduced Features): {accuracy_logistic_reduced:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352ec90a-7aa3-48b4-835d-777495451bcf",
   "metadata": {},
   "source": [
    "B. LDA & QDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "4bd3b2fa-f1ae-46c1-9608-15c4356526fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy (LDA - Reduced Features): 0.6674\n",
      "Accuracy (QDA - Reduced Features): 0.6837\n"
     ]
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
    "\n",
    "# LDA\n",
    "lda_model_reduced = LinearDiscriminantAnalysis()\n",
    "lda_model_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_lda_reduced = lda_model_reduced.predict(X_test_reduced)\n",
    "accuracy_lda_reduced = accuracy_score(y_test, y_pred_lda_reduced)\n",
    "\n",
    "# QDA\n",
    "qda_model_reduced = QuadraticDiscriminantAnalysis()\n",
    "qda_model_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_qda_reduced = qda_model_reduced.predict(X_test_reduced)\n",
    "accuracy_qda_reduced = accuracy_score(y_test, y_pred_qda_reduced)\n",
    "\n",
    "print(f\"Accuracy (LDA - Reduced Features): {accuracy_lda_reduced:.4f}\")\n",
    "print(f\"Accuracy (QDA - Reduced Features): {accuracy_qda_reduced:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa94c5f-65c1-4215-bfd6-25045f778797",
   "metadata": {},
   "source": [
    "C. Ridge & Lasso Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "36bcdedb-4176-4ad4-83ad-d4bf08765387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Î» (Ridge - Reduced Features): 1.0\n",
      "Best Î» (Lasso - Reduced Features): 0.001\n",
      "MSE (Ridge - Reduced Features): 0.2105\n",
      "MSE (Lasso - Reduced Features): 0.2105\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "lambda_grid = np.logspace(-3, 3, 5) \n",
    "\n",
    "ridge_reduced = RidgeCV(alphas=lambda_grid, cv=5)\n",
    "ridge_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_ridge_reduced = ridge_reduced.predict(X_test_reduced)  # Ensure using the reduced test set\n",
    "mse_ridge_reduced = mean_squared_error(y_test, y_pred_ridge_reduced)  # âœ… Define MSE\n",
    "\n",
    "lasso_reduced = LassoCV(alphas=lambda_grid, cv=5, max_iter=5000)\n",
    "lasso_reduced.fit(X_train_reduced, y_train)\n",
    "y_pred_lasso_reduced = lasso_reduced.predict(X_test_reduced)  # Ensure using the reduced test set\n",
    "mse_lasso_reduced = mean_squared_error(y_test, y_pred_lasso_reduced)  # âœ… Define MSE\n",
    "\n",
    "\n",
    "print(f\"Best Î» (Ridge - Reduced Features): {ridge_reduced.alpha_}\")\n",
    "print(f\"Best Î» (Lasso - Reduced Features): {lasso_reduced.alpha_}\")\n",
    "print(f\"MSE (Ridge - Reduced Features): {mse_ridge_reduced:.4f}\")  # âœ… Now correctly defined\n",
    "print(f\"MSE (Lasso - Reduced Features): {mse_lasso_reduced:.4f}\")  # âœ… Now correctly defined\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "8a67c0de-e1f4-4fa0-bcee-8e384d69faaf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>MSE / Accuracy (All Features)</th>\n",
       "      <th>MSE / Accuracy (Reduced Features)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>0.147626</td>\n",
       "      <td>0.210480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.796817</td>\n",
       "      <td>0.684503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LDA</td>\n",
       "      <td>0.730979</td>\n",
       "      <td>0.667446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>QDA</td>\n",
       "      <td>0.725866</td>\n",
       "      <td>0.683712</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ridge</td>\n",
       "      <td>0.147542</td>\n",
       "      <td>0.210480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Lasso</td>\n",
       "      <td>0.149553</td>\n",
       "      <td>0.210512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  MSE / Accuracy (All Features)  \\\n",
       "0    Linear Regression                       0.147626   \n",
       "1  Logistic Regression                       0.796817   \n",
       "2                  LDA                       0.730979   \n",
       "3                  QDA                       0.725866   \n",
       "4                Ridge                       0.147542   \n",
       "5                Lasso                       0.149553   \n",
       "\n",
       "   MSE / Accuracy (Reduced Features)  \n",
       "0                           0.210480  \n",
       "1                           0.684503  \n",
       "2                           0.667446  \n",
       "3                           0.683712  \n",
       "4                           0.210480  \n",
       "5                           0.210512  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  MSE / Accuracy (All Features)  \\\n",
      "0    Linear Regression                       0.147626   \n",
      "1  Logistic Regression                       0.796817   \n",
      "2                  LDA                       0.730979   \n",
      "3                  QDA                       0.725866   \n",
      "4                Ridge                       0.147542   \n",
      "5                Lasso                       0.149553   \n",
      "\n",
      "   MSE / Accuracy (Reduced Features)  \n",
      "0                           0.210480  \n",
      "1                           0.684503  \n",
      "2                           0.667446  \n",
      "3                           0.683712  \n",
      "4                           0.210480  \n",
      "5                           0.210512  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "model_comparison = pd.DataFrame({\n",
    "    \"Model\": [\"Linear Regression\", \"Logistic Regression\", \"LDA\", \"QDA\", \"Ridge\", \"Lasso\"],\n",
    "    \"MSE / Accuracy (All Features)\": [mse_linear, accuracy_logistic, accuracy_lda_pca, accuracy_qda_pca, mse_ridge, mse_lasso],\n",
    "    \"MSE / Accuracy (Reduced Features)\": [mse_linear_reduced, accuracy_logistic_reduced, accuracy_lda_reduced, accuracy_qda_reduced, mse_ridge_reduced, mse_lasso_reduced]\n",
    "})\n",
    "\n",
    "display(model_comparison)\n",
    "print(model_comparison)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac429d2f-e263-46c9-993a-a3a1fcf3eb27",
   "metadata": {},
   "source": [
    "## **Effects of Feature Reduction on Model Performance**\n",
    "\n",
    "### **Summary of Model Performance**\n",
    "| Model | MSE / Accuracy (All Features) | MSE / Accuracy (Reduced Features) |\n",
    "|--------|--------------------------|--------------------------|\n",
    "| **Linear Regression** | **0.1476** (MSE) | **0.2105** (MSE) |\n",
    "| **Logistic Regression** | **79.68%** | **68.45%** |\n",
    "| **LDA** | **73.10%** | **66.74%** |\n",
    "| **QDA** | **72.59%** | **68.37%** |\n",
    "| **Ridge** | **0.1475** (MSE) | **0.2105** (MSE) |\n",
    "| **Lasso** | **0.1496** (MSE) | **0.2105** (MSE) |\n",
    "\n",
    "---\n",
    "\n",
    "### **Main Insights**\n",
    "1. **Linear Regression, Ridge, and Lasso Regression showed decreased performance following feature reduction**  \n",
    "   - **MSE rose notably** with fewer predictors (from **0.1476** to **0.2105**).  \n",
    "   - This indicates that **key features were excluded**, which compromised the model's accuracy.  \n",
    "   - **Regularization methods (Ridge & Lasso) did not significantly mitigate** the increase in errors.\n",
    "\n",
    "2. **Logistic Regression, LDA, and QDA also experienced a decline in accuracy**  \n",
    "   - Logistic Regression's accuracy **fell from 79.68% to 68.45%**.  \n",
    "   - LDA and QDA saw minor accuracy drops, suggesting that removing features impacted classification performance.\n",
    "\n",
    "3. **QDA fared better with feature reduction compared to LDA and Logistic Regression**  \n",
    "   - QDA **maintained an accuracy of 68.37%**, indicating its adaptability to a reduced feature space.  \n",
    "   - This may be attributed to **its capacity to manage various covariance structures** effectively.\n",
    "\n",
    "---\n",
    "\n",
    "### **Analysis of Findings**\n",
    "Reducing features hurt all models, which implies that the omitted variables contained significant information.\n",
    "The significant rise in MSE indicates that linear models (Ridge, Lasso, and Linear Regression) are susceptible to feature selection.\n",
    "- **Classification models (Logistic Regression, LDA, QDA) remained functional but at lower accuracy**, suggesting that some removed predictors were crucial for differentiating cancellations.\n",
    "\n",
    "---\n",
    "\n",
    "### **Concluding Remarks**\n",
    "- **While reducing features can simplify models, it may also hinder accuracy.**\n",
    "Lasso regression can still be advantageous for identifying essential predictors, but it should be applied judiciously.\n",
    "Regularization methods (Ridge and Lasso) also experienced declines in performance, indicating that every predictor plays a valuable role.\n",
    "Employing a feature selection process with Lasso or leveraging domain knowledge might enhance outcomes rather than utilizing random feature reduction.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df20c12-e18a-445f-83ab-3db7ad963932",
   "metadata": {},
   "source": [
    "## **Summary & Final Observations**\n",
    "This project investigated various statistical and machine-learning techniques for forecasting hotel booking cancellations. Linear regression, Logistic Regression, LDA, QDA, Ridge, and Lasso Regression were assessed, analyzing their effectiveness with **all predictors** and following **feature reduction**.\n",
    "\n",
    "---\n",
    "\n",
    "### **ðŸ”¹ Main Discoveries**\n",
    "1. **Logistic Regression emerged as the highest-performing classification model (79.68% accuracy).**  \n",
    "   - It successfully predicted cancellations and allowed for **threshold adjustments**, making it beneficial for business strategies.  \n",
    "   - Modifying the probability threshold assisted in balancing **false positives and false negatives**, mitigating overbooking risks.\n",
    "\n",
    "2. **LDA and QDA showed decent performances, but Logistic Regression surpassed them.**  \n",
    "   Assuming linear decision barriers, LDA achieved a reasonable outcome (**73.10% accuracy**).  \n",
    "   - QDA performed slightly worse (**72.59% accuracy**), indicating that a quadratic boundary did not significantly enhance results for this dataset.\n",
    "\n",
    "3. **Ridge and Lasso Regression offered insights but were less effective for classification.**  \n",
    "   - Ridge kept all the features intact with a **lower MSE (0.1475)**.  \n",
    "   - Lasso engaged in feature selection, narrowing the model down to **27 significant predictors**, though it produced a small increase in MSE (**0.1496**).\n",
    "\n",
    "4. **Feature reduction had a detrimental effect on all models.**  \n",
    "   - **Linear Regression, Ridge, and Lasso experienced notable MSE hikes**, suggesting that the omitted features held essential information.  \n",
    "   - **Classification models (Logistic Regression, LDA, QDA) recorded drops in accuracy**, highlighting the necessity of informed feature selection.\n",
    "\n",
    "---\n",
    "\n",
    "### **Final Suggestions**\n",
    "Logistic regression is the optimal choice for enhanced prediction accuracy, Logistic Regression stands out as the optimal choice**, particularly with threshold modifications to lessen overbooking risks.\n",
    "- **For clarity & feature selection:** **Lasso Regression aids in pinpointing crucial predictors**, making it advantageous for streamlining models while preserving performance.\n",
    "For stability in high-dimensional datasets, Ridge Regression is beneficial when multicollinearity poses a problem**, but it does not automatically select features.\n",
    "- **Feature selection should be informed by statistical methods (like Lasso) or business insight**, rather than arbitrary reductions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Business Insight**\n",
    "Utilizing Logistic Regression with adjusted thresholds, the hotel organization can effectively reduce the risk of overbooking while ensuring accurate cancellation predictions. Furthermore, Lasso Regression can facilitate feature selection** to uncover significant factors affecting cancellations, enabling better strategic choices.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570a99cc-d573-4815-8a9f-c5adb7d9e655",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
